
# PUNTO 1 ‚Äî Web Scrapping

# 1. Introducci√≥n General

La **Universidad Santo Tom√°s** se encuentra en un proceso de modernizaci√≥n tecnol√≥gica, buscando optimizar sus flujos de trabajo mediante soluciones basadas en **inteligencia artificial**, **visi√≥n por computador** y **automatizaci√≥n**.

Dentro de este proceso, se solicit√≥ el dise√±o e implementaci√≥n de un sistema capaz de:

* Construir una **base de datos extensa** con im√°genes de elementos de laboratorio de electr√≥nica.
* Automatizar el proceso de adquisici√≥n mediante t√©cnicas de **web scraping**. 
* Utilizar **concurrencia** (hilos), **sem√°foros**, **exclusi√≥n mutua** y **colas seguras** para incrementar el rendimiento.
* Generar **metadatos consistentes** para su posterior uso en modelos con MediaPipe.

---

Este informe documenta en profundidad la **arquitectura del *software***, los fundamentos te√≥ricos empleados y las **decisiones t√©cnicas** que guiaron el desarrollo.

El √©nfasis est√° en la implementaci√≥n de **hilos**, **sem√°foros**, **mutex** y **sincronizaci√≥n concurrente**.

---
# 2. Justificaci√≥n del uso de Web Scraping con Concurrencia

## 2.1. La necesidad de scraping masivo

Cada clase de objeto deb√≠a contar con **m√≠nimo 200 im√°genes**, y el proyecto inclu√≠a inicialmente **10 clases** (mult√≠metro, osciloscopio, protoboard, caut√≠n, fuente de poder, generador de funciones, motor paso a paso, transformador, resistencia, capacitor).

Esto implicaba recolectar **2000 im√°genes v√°lidas**, no duplicadas, descargadas, verificadas y almacenadas en el menor tiempo posible.

El *scraping* secuencial se consider√≥ inviable por las siguientes razones:
* **Alt√≠simos tiempos de espera** (latencia de red, en este caso mas de 10 horas para dos clases).
* Dependencia de redes externas lentas.
* Necesidad de **validar im√°genes manualmente** (proceso lento).
* **Riesgo alto de bloqueo** por parte del motor de b√∫squeda (por la alta frecuencia de peticiones desde un √∫nico hilo).

> 

La **soluci√≥n √≥ptima** para cumplir con los requisitos de volumen y tiempo deb√≠a ser inherentemente **concurrente**.

---

## 3. Fundamentos Te√≥ricos de Concurrencia Aplicados

Esta secci√≥n profundiza en la teor√≠a que fundamenta la implementaci√≥n utilizada.
# 3.1. Hilos (Threads)

Los **hilos** (`Threads`) permiten ejecutar m√∫ltiples tareas de forma **concurrente** dentro de un mismo proceso, compartiendo memoria, variables y archivos.

### Aplicaci√≥n en este Proyecto
En este proyecto de *web scraping* masivo:
* Cada hilo es un **consumidor** de la cola de URLs.
* Se encarga de descargar una imagen, verificarla, procesarla, guardarla y registrar sus metadatos.

### Ventajas de usar hilos en Web Scraping
El uso de hilos fue cr√≠tico debido a que el *scraping* es una tarea limitada por operaciones de Entrada/Salida (**I/O-bound**):

| Ventaja | Descripci√≥n |
| :--- | :--- |
| **Alta latencia de red** | Mientras un hilo espera a que se complete una descarga de red, otros pueden avanzar. |
| **CPU poco utilizada** | El proceso est√° limitado por la I/O, no por procesamiento de CPU. |
| **Bajo consumo de memoria** | Comparado con la creaci√≥n de m√∫ltiples procesos independientes (*multiprocessing*). |
| **Simplicidad de estado** | Es m√°s sencillo compartir y sincronizar el estado global (contador por clase, CSV de metadatos, cola de URLs). |

### Justificaci√≥n 
El *scraping* es una tarea t√≠picamente I/O-bound, lo cual se beneficia del modelo de hilos incluso con la presencia del **GIL (Global Interpreter Lock)** de Python, ya que:

* El **GIL se libera** durante operaciones de red que esperan datos.
* La descarga de im√°genes no compite por tiempo de CPU.
* Los hilos se ejecutan de manera eficiente sin necesidad de recurrir a la complejidad del *multiprocessing*.

---

# 3.2. Exclusi√≥n Mutua ‚Äî Mutex / Lock

**Problema:** Los hilos comparten recursos sensibles, como:

* El archivo **`metadata.csv`** (para registrar los datos).
* Los **contadores de im√°genes** por categor√≠a (para nombrar archivos).
* Los √≠ndices para nombres de archivos.
* La carpeta donde se guardan las im√°genes.

Sin protecci√≥n, aparece el riesgo de **condiciones de carrera** (*race conditions*).

### üö® Ejemplo de Condici√≥n de Carrera real:

Dos hilos revisan simult√°neamente cu√°ntas im√°genes lleva la categor√≠a "multimeter":

```bash
Hilo 1 ‚Üí ve que hay 50
Hilo 2 ‚Üí ve que hay 50
Ambos deciden guardar la imagen como multimeter_00051.jpg
```
- Resultado: Se sobrescribe un archivo o se corrompe el dataset.

- Soluci√≥n:

  - Se implement√≥:
    ```bash
    counter_lock = threading.Lock()
    ```
  - Este mutex garantiza:

    - Acceso exclusivo a las secciones cr√≠ticas.

    - Escritura ordenada en metadata.csv.

    - Que solo un hilo manipule los contadores.

    - Generaci√≥n correcta de nombres de archivos.

    - Evitar duplicados generados por carreras.

  # 3.3. Semaforizaci√≥n ‚Äî Control de Conexiones Simult√°neas

Los **Sem√°foros** se emplean para **limitar el n√∫mero de conexiones abiertas simult√°neamente** a la red, actuando como un contador de permisos.

### Implementaci√≥n:
Se utiliz√≥ un sem√°foro para restringir cu√°ntos hilos pueden estar descargando activamente en un momento dado:

```bash
download_semaphore = threading.Semaphore(MAX_SIMULTANEOUS_DOWNLOADS)
```

- ¬øPor qu√© era necesario un Sem√°foro?

  - El sem√°foro es crucial para la robustez del scraper debido a los siguientes riesgos:

      - Bloqueo de IPs: Los servicios de im√°genes (como Bing) bloquean o imponen CAPTCHAs a IPs que realizan descargas excesivas en paralelo en un corto per√≠odo de tiempo.

      - Patr√≥n sospechoso: Sin l√≠mite, 8 hilos podr√≠an generar 8 conexiones simult√°neas cada segundo, lo que se identifica como un patr√≥n de ataque o bot.

      - Saturaci√≥n: El sistema podr√≠a saturar la red local o el motor de b√∫squeda, provocando timeouts continuos y un rendimiento inestable.

      - Funci√≥n: El sem√°foro pone en espera a los hilos que exceden el l√≠mite, asegurando que solo un n√∫mero controlado (MAX_SIMULTANEOUS_DOWNLOADS) acceda a los recursos de red a la vez.

  ### **C√≥mo funciona**

    - Antes de descargar una imagen:
    ```bash
    download_semaphore.acquire()
    ```
    Cuando finaliza:
    ```bash
    download_semaphore.release()
    ```
  ### **Analizando el impacto**

- Al limitar a 5 conexiones simult√°neas se logr√≥:

  - Mantener tr√°fico estable.

  - Evitar bloqueos por parte del servidor.

  - Permitir a los dem√°s hilos continuar descargando en paralelo.

# 3.4. Cola de Tareas ‚Äî Productor/Consumidor

La arquitectura se dise√±√≥ como:

- Productor

  - Produce URLs obtenidas con Selenium.

  - Las inserta en la cola:
```bash
download_queue.put((url, label))
```
- Consumidores

  - Hilos que obtienen tareas:
```bash
item = download_queue.get()
```
  - Procesan la descarga, verificaci√≥n, y guardado.

 ## 4. Arquitectura del Sistema Desarrollado

La arquitectura final del sistema de extracci√≥n y descarga se estructur√≥ en los siguientes **m√≥dulos** principales:

---

### 1. M√≥dulo Selenium

Este m√≥dulo es el Productor de tareas, encargado de la extracci√≥n de las URLs objetivo.

* **Funci√≥n:** Extrae cientos de URLs tanto de miniaturas como de las im√°genes de alta definici√≥n (reales).
* **Mecanismo:** Se desplaz√≥ din√°micamente por la plataforma **Bing Images**.
* **T√©cnica Clave:** Aprovecha el evento de **click en las miniaturas** para exponer y obtener las URLs de las im√°genes en calidad HD.

---

### 2. M√≥dulo de Cola de URLs

Act√∫a como el buffer central entre los m√≥dulos de extracci√≥n y los de descarga.

* **Funci√≥n Principal:** Administra el **backlog de trabajo** (las URLs extra√≠das).
* **Control de Flujo:** Controla el ritmo y el flujo de las tareas que ser√°n entregadas a los **hilos consumidores**.

---

### 3. M√≥dulo Multithreading

Este m√≥dulo implementa la concurrencia para la descarga eficiente de archivos.

* **Funci√≥n:** Ejecuta **m√∫ltiples descargas en paralelo** para maximizar el rendimiento.
* **Principio:** Cada hilo de trabajo opera de forma **independiente y segura** al procesar su tarea asignada.

---

### 4. Gestores de Sincronizaci√≥n

Componentes esenciales para garantizar la seguridad y el orden en el entorno concurrente.

* **`Lock`:** Se utiliza para **proteger datos compartidos** de condiciones de carrera (ej., al actualizar un contador global).
* **`Sem√°foro`:** Se emplea para **controlar las conexiones simult√°neas** a la fuente externa, evitando la sobrecarga o el bloqueo por parte del servidor de destino.

---

### 5. M√≥dulo de Metadatos

Asegura la trazabilidad y la integridad de los datos descargados.

* **Almacenamiento:** Guarda la informaci√≥n esencial de cada imagen: **nombre, tama√±o, URL y el hash SHA256**.
* **Beneficios:**
    * Asegura la **reproducibilidad** del proceso.
    * Permite la **detecci√≥n futura de duplicados** de manera infalible.

---

### 6. Depuradores Posteriores

Scripts complementarios ejecutados tras la finalizaci√≥n de las descargas.

* **Funci√≥n 1:** **Eliminaci√≥n de im√°genes corruptas** o incompletas.
* **Funci√≥n 2:** Realiza la **deduplicaci√≥n** final por hash (`SHA256`) utilizando scripts de apoyo.

## 5. Problemas Reales Durante el Desarrollo y Soluciones

Esta secci√≥n detalla los principales obst√°culos encontrados durante la implementaci√≥n y las soluciones t√©cnicas aplicadas, lo que demuestra el cumplimiento de objetivos y el aprendizaje t√©cnico adquirido.

---

## 5.1. Descarga de im√°genes irrelevantes

El principal desaf√≠o en la fase de extracci√≥n fue la **baja precisi√≥n** de los resultados de b√∫squeda de la fuente (`Bing Images`).

* **Problema Real:** Al buscar un t√©rmino t√©cnico y espec√≠fico como **"multimeter"** (mult√≠metro), la herramienta de b√∫squeda tend√≠a a devolver im√°genes contextualmente irrelevantes, tales como sillas, escritorios o fotograf√≠as de personas utilizando el mult√≠metro, en lugar del dispositivo en s√≠.
* **Soluciones Aplicadas:**
    1.  **Ajuste del Keyword:** Se implement√≥ una estrategia de **ajuste fino de los t√©rminos de b√∫squeda** para intentar acotar los resultados.
    2.  **Curado en Limpieza Posterior:** Se asumi√≥ una fase de **curado manual o semiautom√°tico** como parte del proceso de limpieza posterior para descartar im√°genes no deseadas.
    3.  **Balance de Dataset:** Posteriormente, para diversificar y mejorar la calidad del conjunto de datos, se a√±adi√≥ el t√©rmino **"transistor"** a la lista de keywords, buscando **balancear** la tipolog√≠a de las im√°genes.

---

### 5.2. Tiempo excesivo de Extracci√≥n (>10 horas)

La optimizaci√≥n del tiempo de ejecuci√≥n fue cr√≠tica, ya que el proceso inicial consum√≠a una cantidad de tiempo inaceptable para la escala de datos requerida.

* **Problema Real:** La extracci√≥n de aproximadamente **2000 im√°genes limpias** requiri√≥ un tiempo de ejecuci√≥n excesivamente largo, lo que afect√≥ la productividad y la iteraci√≥n del desarrollo:
    * **5 horas** con Firefox (El proceso fall√≥ por errores de perfil del navegador).
    * **M√°s de 10 horas en total** para completar las extracciones de solo dos clases con la implementaci√≥n inicial de **Selenium**.
* **Intentos de Soluci√≥n Fallidos:**
    * Se intent√≥ alternar el *driver* de Selenium entre **Chromium** y **Firefox** para buscar una ganancia de rendimiento, sin √©xito significativo.
    * Se evaluaron m√©todos externos como la librer√≠a **`bing_image_downloader`**, pero se descartaron por falta de flexibilidad o control.
* **Soluci√≥n Final Adoptada (Combinaci√≥n de Enfoques):**
    1.  **Scraping Multithreading:** Se implement√≥ y optimiz√≥ un sistema de **scraping concurrente** utilizando **`multithreading`** para manejar la mayor√≠a de las descargas en paralelo.
    2.  **Herramienta Alternativa Espec√≠fica:** Se utiliz√≥ una **herramienta alternativa espec√≠fica** para la extracci√≥n del subconjunto de im√°genes de **"transistores"**, aprovechando su eficiencia para esa tarea concreta.
    3.  **Limpieza Posterior Autom√°tica:** La dependencia en una **limpieza posterior autom√°tica** se increment√≥ para manejar la escala de datos extra√≠dos r√°pidamente, compensando la velocidad de extracci√≥n con un proceso de filtrado robusto.

### 5.3. Eliminaci√≥n masiva ‚Äî P√©rdida del 40‚Äì60% de im√°genes

Despu√©s del dedupe por hash:
  ```bash
  Eliminados: 1189
  ```
- Causas:

  - Im√°genes duplicadas en miniaturas/HD.

  - Servidores devolv√≠an la misma imagen con URLs diferentes.

  - Historias de cache del buscador.

- Resultado final:

  - Todas las carpetas quedaron con m√°s de 100 im√°genes v√°lidas.
  - Aunque no se alcanz√≥ exactamente 200 por clase, el dataset es consistente y limpio.

  ## 7. Conclusiones del Punto 1: Logros y Aprendizajes

La ejecuci√≥n exitosa de este proyecto de construcci√≥n de dataset y sistema de scraping condujo a los siguientes logros y aprendizajes clave:

---

### Logros del Proyecto

* **Construcci√≥n de un Dataset Personalizado para el Laboratorio:** Se logr√≥ crear un dataset de alta calidad, curado y espec√≠fico, con una cantidad de m√°s de 100 im√°genes por clase despu√©s de la fase de limpieza y depuraci√≥n.
* **Desarrollo de un Sistema de Scraping Robusto y Realista:** Se dise√±√≥ y codific√≥ un sistema de extracci√≥n que demostr√≥ ser capaz de realizar trabajo intensivo de larga duraci√≥n, resolviendo desaf√≠os reales de estabilidad y gesti√≥n de errores.
* **Implementaci√≥n de T√©cnicas Avanzadas de Concurrencia:** Se aplicaron con √©xito principios de multithreading y sincronizaci√≥n (Lock, Semaphore) en una aplicaci√≥n real, con impactos tangibles en la reducci√≥n del tiempo de procesamiento.

---

### Aprendizajes Clave

* **L√≠mites y Fallos Comunes del Scraping:** Se obtuvo una experiencia pr√°ctica profunda en el manejo y mitigaci√≥n de problemas intr√≠nsecos al web scraping a gran escala, incluyendo:
    * **Bloqueos de IP:** Estrategias para evadir o manejar las restricciones del servidor fuente.
    * **Im√°genes Ruidosas:** Gesti√≥n de im√°genes con contenido contextual irrelevante.
    * **Contenidos No Relevantes:** Filtrado efectivo de resultados que no cumplen con los requisitos de la clase (ej., errores de keyword).
    * **Duplicados Masivos:** Implementaci√≥n de hashing (SHA256) para la detecci√≥n y eliminaci√≥n eficiente.

* **Generaci√≥n de una Arquitectura Escalable:** El dise√±o modular y desacoplado del sistema sent√≥ las bases para la escalabilidad y la integraci√≥n futura con m√≥dulos de Machine Learning para los siguientes objetivos del proyecto:
    * Clasificaci√≥n con MediaPipe.
    * Reconocimiento de elementos.
    * Implementaci√≥n del sistema final en Streamlit.

  ---

## üìÅ Estructura del proyecto: tree + explicaci√≥n completa

A continuaci√≥n se muestra la estructura final del proyecto de Web Scraping con Python, enriquecida con una explicaci√≥n exhaustiva de cada componente:

**webscrapping/**
* **venv/**
    * ... (Entorno virtual con dependencias)
* **dataset/**
    * [Carpetas de Clases]
        * breadboard/
        * capacitor_electronic_component/
        * diode_electronic_component/
        * function_generator/
        * multimeter/
        * oscilloscope/
        * resistor_electronic_component/
        * soldering_iron/
        * stepper_motor/
        * transistor_electronic_component/
* **metadata/**
    * metadata.csv (Registro formal y trazabilidad del dataset)
* **Archivos Ejecutables y Scripts**
    * scraper\_dataset.py (Scraper PRINCIPAL: Multihilo, Sem√°foros, Mutex)
    * fast\_download\_transistor.py (Script alterno/de emergencia)
    * check\_corrupt.py (Script para detectar y registrar im√°genes da√±adas)
    * dedupe\_by\_hash.py (Script para eliminaci√≥n masiva de duplicados por hash SHA-256)
    * README.md (Documentaci√≥n principal del proyecto)

## üß© Explicaci√≥n de las Carpetas y Archivos Principales

A continuaci√≥n se detalla la funci√≥n de cada directorio y archivo clave dentro de la estructura del proyecto.

---

### 1. `venv/` ‚Äî Entorno Virtual üß™

Este directorio es esencial para la gesti√≥n de dependencias del proyecto. 

* **Funci√≥n Principal:** Contiene todas las **dependencias de Python** de forma aislada del sistema operativo principal.
* **Prop√≥sito:**
    * **Evita conflictos de versiones** con librer√≠as o paquetes instalados globalmente en el sistema.
    * Aloja librer√≠as espec√≠ficas utilizadas en el proyecto, como **`requests`**, **`Pillow`**, **`bing_image_downloader`**, **`beautifulsoup4`**, etc.
    * **Garantiza la portabilidad:** Asegura que cualquier desarrollador que ejecute el proyecto tenga **exactamente el mismo entorno** de trabajo.
* **Estatus:** Es una carpeta indispensable para el desarrollo profesional y reproducible de proyectos en Python.

---

### 2. `dataset/` ‚Äî Carpetas con las Im√°genes Finales üíæ

Este directorio almacena la salida principal del proceso de *scraping* y limpieza: el conjunto de datos final.

* **Funci√≥n Principal:** Contiene todas las **clases (categor√≠as)** que componen el *dataset*.
* **Estructura Interna:** Cada clase se representa mediante una subcarpeta dentro de `dataset/`.
* **Nomenclatura:** Las carpetas de clase tienen un **nombre normalizado** para facilitar el procesamiento posterior por modelos de Machine Learning.

- Ejemplos:

  - breadboard/

  - multimeter/

  - transistor_electronic_component/

  Cada carpeta dentro de `dataset/` contiene las siguientes caracter√≠sticas despu√©s del proceso de curado:

* **Im√°genes V√°lidas:** Solo incluye im√°genes que han pasado el proceso de deduplicaci√≥n (sin duplicados).
* **Im√°genes NO Corruptas:** Todos los archivos han sido verificados y garantizan su integridad estructural.
* **Cantidad Final:** **M√°s de 100 im√°genes por clase** despu√©s de la limpieza.

> **Nota:** Aunque el objetivo inicial era de 200 im√°genes por clase, los desaf√≠os inherentes al *scraping* (problemas de precisi√≥n en Bing, el exceso de im√°genes basura y la enorme cantidad de duplicados) redujeron el total final. Esta limitaci√≥n cuantitativa se justifica y explica detalladamente en el **README t√©cnico** del proyecto.

---

### 3. `metadata/metadata.csv` ‚Äî Registro Formal del Dataset üìÑ

Este archivo es crucial para la **trazabilidad, auditor√≠a y reproducibilidad** del conjunto de datos. En proyectos serios de *Machine Learning* y an√°lisis de datos, el registro formal del origen y estado de cada muestra es un requisito clave.



**Campos T√≠picos del `metadata.csv`:**

| Campo | Descripci√≥n |
| :--- | :--- |
| `image_path` | La ruta relativa al archivo dentro de la carpeta `dataset/`. |
| `class` | La categor√≠a o etiqueta a la que pertenece la imagen (ej. 'multimeter', 'transistor'). |
| `resolution` | La resoluci√≥n de la imagen (ej. '640x480'). |
| `file_size` | El tama√±o del archivo en bytes. |
| `hash_sha256` | El **hash criptogr√°fico SHA256**, fundamental para la detecci√≥n de duplicados y la verificaci√≥n de integridad. |
| `is_corrupt` | Indicador booleano que registra si la imagen fue marcada como corrupta (deber√≠a ser **False** para todas las entradas finales). |
| `duplicate_of` | Si es un duplicado, registra el `image_path` del archivo original que se conserv√≥. |

## 4. `scraper_dataset.py` ‚Äî Scraper PRINCIPAL Multihilo (con Sem√°foros y Mutex)

Este script es el **archivo m√°s importante y central** de todo el proyecto, conteniendo la l√≥gica de concurrencia y la gesti√≥n robusta de errores para la descarga de im√°genes.

---

### Funcionalidades Clave y T√©cnicas de Concurrencia

El script implementa t√©cnicas avanzadas de programaci√≥n concurrente para optimizar el rendimiento y garantizar la integridad de los datos:

* **Uso de Threads (Hilos):**
    * **Prop√≥sito:** Se utilizan para ejecutar la descarga de **m√∫ltiples im√°genes en paralelo**. 
    * **Impacto:** Sin la concurrencia, el proceso de *scraping* habr√≠a tardado un estimado de **40 a 60 horas**.

* **Uso de Sem√°foros (`Semaphore`):**
    * **Funci√≥n:** Se implementa un **sem√°foro** para **limitar el n√∫mero de descargas simult√°neas** a un valor seguro (ejemplo: `semaphore = threading.Semaphore(8)`).
    * **Beneficios:**
        * Evita **bans temporales** por parte de la fuente (`Bing Images`).
        * Previene **errores por saturaci√≥n** del servidor de destino.
        * Minimiza **timeouts masivos** y el riesgo de saturar la CPU o el ancho de banda local.

* **Uso de Mutex (`Lock`):**
    * **Necesidad:** El mutex (o `Lock`) es necesario porque, aunque las im√°genes se descargan en paralelo, **varios hilos deben escribir simult√°neamente** en recursos compartidos, como:
        * El archivo de registro de metadatos (`metadata.csv`).
        * **Contadores globales** de progreso o errores.
    * **Resultado:** El uso del mutex **evita *race conditions*** (condiciones de carrera) y previene la **corrupci√≥n** del archivo CSV, garantizando la escritura at√≥mica de los datos.

---

### Gesti√≥n de Errores y Almacenamiento

El script garantiza la fiabilidad del proceso de descarga mediante control de calidad y robustez:

* **Descarga con Control de Errores Robusto:**
    * **Manejo de Timeouts:** Implementa estrategias de reintento ante fallos de conexi√≥n o tiempos de espera agotados.
    * **Retry Autom√°tico:** Intenta autom√°ticamente la descarga un n√∫mero predefinido de veces antes de marcar una tarea como fallida.
    * **Sanitizaci√≥n del Nombre del Archivo:** Procesa y limpia el nombre del archivo para asegurar la compatibilidad con diferentes sistemas operativos.
    * **Verificaci√≥n de Contenido:** Valida que el archivo descargado sea efectivamente una imagen (ej., contenido tipo `image/jpeg`, `image/png`), descartando posibles archivos HTML o corruptos.

* **Guardado y Organizaci√≥n:** Guarda cada imagen en su **carpeta de clase correspondiente** dentro del directorio `dataset/`, manteniendo la estructura organizada.

### 5. `fast_download_transistor.py` ‚Äî Script Alterno de Emergencia üöÄ

Este script fue desarrollado como una **soluci√≥n de contingencia** para mitigar los problemas de eficiencia y precisi√≥n del *scraper* principal en clases problem√°ticas.

* **Motivaci√≥n:** Se cre√≥ debido a:
    * El tiempo excesivo de ejecuci√≥n del *scraper* principal (**m√°s de 10 horas**).
    * El fallo en completar el objetivo de 200 im√°genes en algunas clases.
    * La alta tasa de **im√°genes irrelevantes** (sillas, autos, etc.) devueltas por Bing.
    * El componente **"transistor"** fue particularmente problem√°tico en la extracci√≥n.

* **Implementaci√≥n:** Utiliza la librer√≠a **`bing_image_downloader`**, pero requiri√≥ una **modificaci√≥n interna del m√≥dulo** debido a:
    * Un **bug** relacionado con la funci√≥n `Path.isdir` en el entorno de desarrollo.
    * La necesidad de **adaptar el flujo de descarga** para integrarlo con la estructura de carpetas del proyecto.

* **Uso:** Solo se emple√≥ una vez para **completar una clase puntual** (la de transistores) y balancear el *dataset*.

---

### 6. `check_corrupt.py` ‚Äî Script para Detectar Im√°genes Da√±adas üõ°Ô∏è

Este script de post-procesamiento garantiza la **integridad y usabilidad** de todos los archivos descargados.

* **Mecanismo de Verificaci√≥n:** Revisa iterativamente cada archivo dentro del directorio `dataset/`.
    * **Proceso:** Intenta abrir la imagen utilizando la librer√≠a **PIL (Pillow)**.
    * **Acci√≥n:** Si la apertura falla, la imagen es marcada como **corrupta** y el estado se **registra en `metadata.csv`**. Opcionalmente, el script puede ser configurado para eliminar el archivo f√≠sicamente.

* **Importancia Cr√≠tica:** Este script fue **crucial** porque:
    * Bing entreg√≥ una cantidad significativamente alta de **im√°genes corruptas** o incompletas.
    * Se detectaron casos de archivos que eran realmente **c√≥digo HTML disfrazado de JPG** (un error com√∫n de *scraping*).

---

### 7. `dedupe_by_hash.py` ‚Äî Eliminaci√≥n Masiva de Duplicados ‚öôÔ∏è

Este script asegura la **unicidad** del *dataset*, un paso fundamental para evitar el sesgo en el entrenamiento de modelos de *Machine Learning*.

* **Proceso Central:**
    * **C√°lculo de Hash:** Calcula el **hash SHA-256** de cada imagen. Esta es la t√©cnica m√°s robusta y **garantiza detectar duplicados** incluso si los archivos tienen nombres o metadatos distintos. 
    * **Eliminaci√≥n:** **Elimina autom√°ticamente los duplicados reales**. En la ejecuci√≥n del proyecto, el resultado fue: **Eliminados: 1189** archivos.

* **Justificaci√≥n de Duplicados:** La alta tasa de duplicados es normal debido a:
    * La repetici√≥n masiva de contenido por parte de la fuente (`Bing`).
    * La similitud entre las clases del *dataset*.
    * La tendencia del buscador a devolver **clones reescalados** de la misma imagen.

* **Registro:** **Actualiza el `metadata.csv`**, marcando cu√°l archivo fue duplicado de cu√°l, manteniendo un registro de la limpieza.
